{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ch09_02_RNN.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jaydenchoe/Fundamentals-of-Deep-Learning-Book/blob/master/ch09_models_for_sequence_analysis/Ch09_02_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PyTorch Primitives for RNN Models"
      ],
      "metadata": {
        "id": "JdVhVjN-llAA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "ZWlouOwETInM"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cell_1 = nn.RNNCell(input_size = 10,\n",
        "                    hidden_size = 20,\n",
        "                    nonlinearity='tanh')\n",
        "\n",
        "cell_2 = nn.LSTMCell(input_size = 10,\n",
        "                     hidden_size = 20)\n",
        "\n",
        "cell_3 = nn.GRUCell(input_size = 10,\n",
        "                    hidden_size = 20)"
      ],
      "metadata": {
        "id": "IWsgDePImGGp"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stacking recurrent units\n",
        "cell_1 = nn.LSTMCell(input_size = 10,\n",
        "                     hidden_size = 20)\n",
        "cell_2 = nn.LSTMCell(input_size = 20,\n",
        "                     hidden_size = 20)\n",
        "\n",
        "full_cell = nn.Sequential(cell_1, cell_2)"
      ],
      "metadata": {
        "id": "czzPCIotmPEU"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run LSTM Cell"
      ],
      "metadata": {
        "id": "7ruPWI82TVHf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input = torch.randn(2, 3, 10) # (time_steps, batch, input_size)\n",
        "hx_init = torch.randn(3, 20) # hidden state of size: (batch_size, hidden_size)\n",
        "# output of output gate\n",
        "cx_init = torch.randn(3, 20) # cell state of size: (batch_size, hidden_size)\n",
        "# output of write gate\n",
        "output = []\n",
        "\n",
        "# loop over time_steps\n",
        "hx, cx = hx_init, cx_init\n",
        "for t in range(input.size()[0]):\n",
        "        hx, cx = cell_1(input[t], (hx, cx)) # input[t] is size (batch_size, input_size)\n",
        "        hx2, cx2 = cell_2(hx, (hx, cx)) # input[t] is size (batch_size, input_size)\n",
        "        output.append(hx2)\n",
        "output = torch.stack(output, dim=0) # shape is (time_steps, batch_size, input_size)"
      ],
      "metadata": {
        "id": "ICdrzWnATYlf"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# multi-layer RNN and LSTM\n",
        "multi_layer_rnn = nn.RNN(input_size = 10,\n",
        "                         hidden_size = 20,\n",
        "                         num_layers = 2,\n",
        "                         nonlinearity = 'tanh')\n",
        "\n",
        "multi_layer_lstm = nn.LSTM(input_size = 10,\n",
        "                           hidden_size = 20,\n",
        "                           num_layers = 2)"
      ],
      "metadata": {
        "id": "noeIWbGtUuX6"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output2 = []\n",
        "\n",
        "# loop over time_steps\n",
        "hx, cx = hx_init, cx_init\n",
        "for t in range(input.size()[0]):\n",
        "        hx, cx = cell_1(input[t], (hx, cx)) # input[t] is size (batch_size, input_size)\n",
        "        hx2, cx2 = cell_2(hx, (hx, cx)) # input[t] is size (batch_size, input_size)\n",
        "        output2.append(hx2)\n",
        "output2 = torch.stack(output2, dim=0) # shape is (time_steps, batch_size, input_size)"
      ],
      "metadata": {
        "id": "CrfO853DUk2j"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.all(output == output2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9wmZH08eVN9M",
        "outputId": "e8816ca8-760a-4b43-c2b8-88aacc3d2087"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(True)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# multi-layer RNN and LSTM with other settings\n",
        "multi_layer_rnn = nn.RNN(input_size = 10,\n",
        "                         hidden_size = 20,\n",
        "                         num_layers = 2,\n",
        "                         nonlinearity = 'tanh',\n",
        "                         batch_first = False,\n",
        "                         dropout = 0.5)\n",
        "\n",
        "multi_layer_lstm = nn.LSTM(input_size = 10,\n",
        "                           hidden_size = 20,\n",
        "                           num_layers = 2,\n",
        "                           batch_first = False,\n",
        "                           dropout = 0.5)"
      ],
      "metadata": {
        "id": "QB1Nwd8dXFNK"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply dropouts\n",
        "# If dropout parameter is non-zero, the model introduces\n",
        "# a Dropout layer on # the outputs of each LSTM layer\n",
        "# except the last layer, with dropout probability equal to dropout.\n",
        "# Default: 0\n",
        "input_size = 32\n",
        "\n",
        "cell_1 = nn.LSTM(input_size,\n",
        "                 hidden_size = 10,\n",
        "                 num_layers=2,\n",
        "                 dropout = 1.0)"
      ],
      "metadata": {
        "id": "HRJSX9fBmPHE"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rnn = nn.LSTM(input_size = 32,\n",
        "              hidden_size = 20,\n",
        "              num_layers = 1,\n",
        "              batch_first= False)\n",
        "\n",
        "inputs = torch.randn((32, 32, 32))\n",
        "output, states = rnn(inputs)"
      ],
      "metadata": {
        "id": "jNQYHSVIRQXZ"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LSTM in action"
      ],
      "metadata": {
        "id": "6FPrCOwxYOwm"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input = torch.randn(5, 3, 10) # (time_steps, batch, input_size)\n",
        "h_0 = torch.randn(2, 3, 20) # (n_layers, batch_size, hidden_size)\n",
        "c_0 = torch.randn(2, 3, 20) # (n_layers, batch_size, hidden_size)\n",
        "\n",
        "rnn = nn.LSTM(10, 20, 2) # (input_size, hidden_size, num_layers)\n",
        "output_n, (hn, cn) = rnn(input, (h_0, c_0))"
      ],
      "metadata": {
        "id": "2u7pR2sxYrdD"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 결과 확인\n",
        "print(\"LSTM 출력 shape:\", output_n.shape)\n",
        "print(\"최종 hidden state shape:\", hn.shape)\n",
        "print(\"최종 cell state shape:\", cn.shape)\n",
        "print(\"첫 번째 timestep의 첫 번째 배치 출력 값 (처음 5개):\", output_n[0, 0, :5])"
      ],
      "metadata": {
        "id": "CaEGEWJRZGK1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66fb3394-f181-440e-9f01-00b2587bfd65"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LSTM 출력 shape: torch.Size([5, 3, 20])\n",
            "최종 hidden state shape: torch.Size([2, 3, 20])\n",
            "최종 cell state shape: torch.Size([2, 3, 20])\n",
            "첫 번째 timestep의 첫 번째 배치 출력 값 (처음 5개): tensor([ 0.1774,  0.0459,  0.2270, -0.0283, -0.1193], grad_fn=<SliceBackward0>)\n"
          ]
        }
      ]
    }
  ]
}