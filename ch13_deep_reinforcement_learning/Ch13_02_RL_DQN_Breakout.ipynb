{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ch13_02_RL_DQN_Breakout.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jaydenchoe/Fundamentals-of-Deep-Learning-Book/blob/master/ch13_deep_reinforcement_learning/Ch13_02_RL_DQN_Breakout.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q-Learning and Deep Q-Networks"
      ],
      "metadata": {
        "id": "zBjJQ8BYnUBZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. 강화학습에 필요한 최신 라이브러리를 모두 설치합니다.\n",
        "!pip install gymnasium[atari] ale-py autorom -q\n",
        "\n",
        "# 2. 설치 적용을 위해 커널(런타임)을 자동으로 재시작합니다. 이후에는 그 다음셀부터 시작해야 합니다.\n",
        "import os\n",
        "os._exit(0)"
      ],
      "metadata": {
        "id": "TWNkwnC6eLOL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Playing Breakout wth DQN\n"
      ],
      "metadata": {
        "id": "kYdVJOj0nYfL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "import gymnasium as gym  # 'gym' 대신 'gymnasium'을 'gym'이라는 별명으로 불러옵니다.\n",
        "import random\n",
        "import numpy as np\n",
        "import copy\n",
        "from PIL import Image\n",
        "import ale_py # Atari(ALE) 환경을 등록하기 위해 추가합니다.\n",
        "\n",
        "# for reproductability\n",
        "torch.manual_seed(0)\n",
        "random.seed(0)\n",
        "\n",
        "print(f'PyTorch: {torch.__version__}')\n",
        "# 출력 메시지도 'Gymnasium'으로 변경해줍니다.\n",
        "print(f'Gymnasium: {gym.__version__}')\n",
        "print(f'Numpy: {np.__version__}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTvbW_dQcHVs",
        "outputId": "0183387b-2a4d-42e0-ca29-24990ae3877f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch: 2.8.0+cu126\n",
            "Gymnasium: 1.2.0\n",
            "Numpy: 2.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def epsilon_greedy_action(action_distribution, epsilon=1e-5):\n",
        "    action_distribution = action_distribution.detach().numpy()\n",
        "    if random.random() < epsilon:\n",
        "        return np.argmax(np.random.random(\n",
        "           action_distribution.shape))\n",
        "    else:\n",
        "        return np.argmax(action_distribution)\n",
        "\n",
        "def epsilon_greedy_action_annealed(action_distribution,\n",
        "                                   percentage,\n",
        "                                   epsilon_start=1.0,\n",
        "                                   epsilon_end=1e-8):\n",
        "    action_distribution = action_distribution.detach().numpy()\n",
        "    annealed_epsilon = epsilon_start*(1.0-percentage) + epsilon_end*percentage\n",
        "    if random.random() < annealed_epsilon:\n",
        "        return np.argmax(np.random.random(\n",
        "          action_distribution.shape))\n",
        "    else:\n",
        "        return np.argmax(action_distribution)"
      ],
      "metadata": {
        "id": "kSMsXEoHcYP4"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EpisodeHistory(object):\n",
        "\n",
        "    def __init__(self):\n",
        "        self.states = []\n",
        "        self.actions = []\n",
        "        self.rewards = []\n",
        "        self.state_primes = []\n",
        "        self.terminals = []\n",
        "\n",
        "    def add_to_history(self, state, action, reward,\n",
        "      state_prime, terminal):\n",
        "        self.states.append(state)\n",
        "        self.actions.append(action)\n",
        "        self.rewards.append(reward)\n",
        "        self.state_primes.append(state_prime)\n",
        "        self.terminals.append(terminal)"
      ],
      "metadata": {
        "id": "pJ2U61P7eOCK"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build DQN Agent"
      ],
      "metadata": {
        "id": "4w_wPjzsicA4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DQNAgent(object):\n",
        "\n",
        "    def __init__(self, num_actions,\n",
        "                 learning_rate=1e-3, history_length=4,\n",
        "                 screen_height=84, screen_width=84,\n",
        "                 gamma=0.99):\n",
        "        self.num_actions = num_actions\n",
        "        self.learning_rate = learning_rate\n",
        "        self.history_length = history_length\n",
        "        self.screen_height = screen_height\n",
        "        self.screen_width = screen_width\n",
        "        self.gamma = gamma\n",
        "\n",
        "        self.build_prediction_network()\n",
        "        self.build_target_network()\n",
        "        #self.build_training()\n",
        "\n",
        "    def build_prediction_network(self):\n",
        "        self.model_predict = nn.Sequential(\n",
        "          nn.Conv2d(4, 32, kernel_size=8 , stride=4),\n",
        "          nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
        "          nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
        "          nn.Flatten(),\n",
        "          nn.Linear(3136, 512),\n",
        "          nn.Linear(512, self.num_actions)\n",
        "          )\n",
        "\n",
        "    def build_target_network(self):\n",
        "        self.model_target = nn.Sequential(\n",
        "          nn.Conv2d(4, 32, kernel_size=8 , stride=4),\n",
        "          nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
        "          nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
        "          nn.Flatten(),\n",
        "          nn.Linear(3136, 512),\n",
        "          nn.Linear(512, self.num_actions)\n",
        "          )\n",
        "\n",
        "    def sample_and_train_pred(self, replay_table, batch_size):\n",
        "\n",
        "        s_t, action, reward, s_t_plus_1, terminal = replay_table.sample_batch(\n",
        "              batch_size)\n",
        "\n",
        "        # given state_t, find q_t (predict_model) and q_t+1 (target_model)\n",
        "        # do it in batches\n",
        "        # Find q_t_plus_1\n",
        "        input_t = torch.from_numpy(s_t_plus_1).float()\n",
        "        model_t = self.model_target.float()\n",
        "        q_t_plus_1 = model_t(input_t)\n",
        "\n",
        "        terminal = torch.tensor(terminal).float()\n",
        "        max_q_t_plus_1, _ = torch.max(q_t_plus_1, dim=1)\n",
        "        reward = torch.from_numpy(reward).float()\n",
        "        target_q_t = (1. - terminal) * self.gamma * max_q_t_plus_1 + reward\n",
        "\n",
        "        # Find q_t, and q_of_action\n",
        "        input_p = torch.from_numpy(s_t).float()\n",
        "        model_p = self.model_predict.float()\n",
        "        q_t = model_p(input_p)\n",
        "        action = torch.from_numpy(action)\n",
        "        action_one_hot = nn.functional.one_hot(action,\n",
        "                                               self.num_actions)\n",
        "        q_of_action = torch.sum(q_t * action_one_hot)\n",
        "\n",
        "        # Compute loss\n",
        "        self.delta = (target_q_t - q_of_action)\n",
        "        self.loss = torch.mean(self.delta)\n",
        "\n",
        "        # Update predict_model gradients (only)\n",
        "        self.optimizer = optim.Adam(self.model_predict.parameters(),\n",
        "                                    lr = self.learning_rate)\n",
        "        self.loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        return q_t\n",
        "\n",
        "    def predict_action(self, state, epsilon_percentage):\n",
        "        input_p = torch.from_numpy(state).float().unsqueeze(dim=0)\n",
        "        model_p = self.model_predict.float()\n",
        "        action_distribution = model_p(input_p)\n",
        "        # sample from action distribution\n",
        "        action = epsilon_greedy_action_annealed(action_distribution.detach(),\n",
        "                                                epsilon_percentage)\n",
        "        return action\n",
        "\n",
        "    def process_state_into_stacked_frames(self,\n",
        "                                          frame,\n",
        "                                          past_frames,\n",
        "                                          past_state=None):\n",
        "        full_state = np.zeros((self.history_length,\n",
        "                              self.screen_width,\n",
        "                              self.screen_height))\n",
        "\n",
        "        if past_state is not None:\n",
        "            for i in range(len(past_state)-1):\n",
        "                full_state[i, :, :] = past_state[i+1, :, :]\n",
        "            full_state[-1, :, :] = self.preprocess_frame(frame,\n",
        "                                                        (self.screen_width,\n",
        "                                                          self.screen_height)\n",
        "                                                        )\n",
        "        else:\n",
        "            all_frames = past_frames + [frame]\n",
        "            for i, frame_f in enumerate(all_frames):\n",
        "                full_state[i, :, :] = self.preprocess_frame(frame_f,\n",
        "                                                            (self.screen_width,\n",
        "                                                            self.screen_height)\n",
        "                                                            )\n",
        "        return full_state\n",
        "\n",
        "    def to_grayscale(self, x):\n",
        "        return np.dot(x[...,:3], [0.299, 0.587, 0.114])\n",
        "\n",
        "    def preprocess_frame(self, im, shape):\n",
        "        cropped = im[16:201,:] # (185, 160, 3)\n",
        "        grayscaled = self.to_grayscale(cropped) # (185, 160)\n",
        "        # resize to (84,84)\n",
        "        resized = np.array(Image.fromarray(grayscaled).resize(shape))\n",
        "        mean, std = 40.45, 64.15\n",
        "        frame = (resized-mean)/std\n",
        "        return frame"
      ],
      "metadata": {
        "id": "Zqdz6C0OehUJ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementing Experience Replay"
      ],
      "metadata": {
        "id": "O06XH_jpilB-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ExperienceReplayTable(object):\n",
        "\n",
        "    def __init__(self, table_size=50000):\n",
        "        self.states = []\n",
        "        self.actions = []\n",
        "        self.rewards = []\n",
        "        self.state_primes = []\n",
        "        self.terminals = []\n",
        "\n",
        "        self.table_size = table_size\n",
        "\n",
        "    def add_episode(self, episode):\n",
        "        self.states += episode.states\n",
        "        self.actions += episode.actions\n",
        "        self.rewards += episode.rewards\n",
        "        self.state_primes += episode.state_primes\n",
        "        self.terminals += episode.terminals\n",
        "\n",
        "        self.purge_old_experiences()\n",
        "\n",
        "    def purge_old_experiences(self):\n",
        "        while len(self.states) > self.table_size:\n",
        "            self.states.pop(0)\n",
        "            self.actions.pop(0)\n",
        "            self.rewards.pop(0)\n",
        "            self.state_primes.pop(0)\n",
        "\n",
        "    def sample_batch(self, batch_size):\n",
        "        s_t, action, reward, s_t_plus_1, terminal = [], [], [], [], []\n",
        "        rands = np.arange(len(self.states))\n",
        "        np.random.shuffle(rands)\n",
        "        rands = rands[:batch_size]\n",
        "\n",
        "        for r_i in rands:\n",
        "            s_t.append(self.states[r_i])\n",
        "            action.append(self.actions[r_i])\n",
        "            reward.append(self.rewards[r_i])\n",
        "            s_t_plus_1.append(self.state_primes[r_i])\n",
        "            terminal.append(self.terminals[r_i])\n",
        "        return (np.array(s_t), np.array(action), np.array(reward),\n",
        "                np.array(s_t_plus_1), np.array(terminal))"
      ],
      "metadata": {
        "id": "y7LaP4DHipXw"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set up Q Learning\n",
        "- The configuration values below are for illustrative purposes so you can execute the code in Colab. Training takes a long time (days) at realistic values.\n",
        "- A larger reward value for Breakout may require max_episode_length to be 100000.  That is, you need to be able to play long enough to get a decent reward (score)."
      ],
      "metadata": {
        "id": "EkSGh7c_hEUn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learn_start = 4\n",
        "total_episodes = 32\n",
        "epsilon_stop = 32\n",
        "train_frequency = 2\n",
        "target_frequency = 4\n",
        "batch_size = 4\n",
        "max_episode_length = 1000\n",
        "\n",
        "# 'Breakout-v4' 대신 Gymnasium의 공식 ID인 'ALE/Breakout-v5'를 사용합니다.\n",
        "env = gym.make('ALE/Breakout-v5')\n",
        "num_actions = env.action_space.n\n",
        "solved = False\n",
        "\n",
        "print(\"✅ Breakout 환경 생성 완료!\")\n",
        "print(f\"Action space size: {num_actions}\")"
      ],
      "metadata": {
        "id": "w3VlHF6m3GjG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27722002-47bc-4367-aca3-4d6c9ff30d81"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Breakout 환경 생성 완료!\n",
            "Action space size: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent = DQNAgent(num_actions=num_actions,\n",
        "                 learning_rate=1e-4,\n",
        "                 history_length=4,\n",
        "                 gamma=0.98)"
      ],
      "metadata": {
        "id": "yv5hfhFBhO7p"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train DQN"
      ],
      "metadata": {
        "id": "VgkJn_-8h_EF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "episode_rewards = []\n",
        "q_t_list = []\n",
        "batch_losses = []\n",
        "past_frames_last_time = None\n",
        "\n",
        "replay_table = ExperienceReplayTable()\n",
        "global_step_counter = 0\n",
        "\n",
        "for i in range(total_episodes):\n",
        "    # Get initial frame -> state\n",
        "    # 수정 1: reset()이 2개의 값을 반환하므로 두 변수로 받습니다.\n",
        "    frame, info = env.reset()\n",
        "    # past_frames is a list of past 3 frames (np.arrays)\n",
        "    past_frames = [copy.deepcopy(frame) for _ in range(agent.history_length-1)]\n",
        "    state = agent.process_state_into_stacked_frames(\n",
        "        frame, past_frames, past_state=None) # state is (4,84,84)\n",
        "\n",
        "    # initialize episode history (s_t, a, r, s_t+1, terminal)\n",
        "    episode_reward = 0.0\n",
        "    episode_history = EpisodeHistory()\n",
        "    epsilon_percentage = float(min(i/float(epsilon_stop), 1.0))\n",
        "\n",
        "    for j in range(max_episode_length):\n",
        "        # predict action or choose random action at first\n",
        "        if global_step_counter < learn_start:\n",
        "          action = np.argmax(np.random.random((agent.num_actions)))\n",
        "        else:\n",
        "          action = agent.predict_action(state, epsilon_percentage)\n",
        "\n",
        "        # take action, get next frame (-> next state), reward, and terminal\n",
        "        # 수정 2: step()이 5개의 값을 반환하므로 5개의 변수로 받습니다.\n",
        "        frame_prime, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "        # 수정 3: 기존 'terminal' 변수를 새로운 API에 맞게 정의합니다.\n",
        "        terminal = terminated or truncated\n",
        "\n",
        "        if terminal == True:\n",
        "          reward -= 1\n",
        "\n",
        "        # get next state from next frame and past frames\n",
        "        state_prime = agent.process_state_into_stacked_frames(frame_prime,\n",
        "                                                              past_frames,\n",
        "                                                              past_state=state)\n",
        "        # Update past_frames with frame_prime for next time\n",
        "        past_frames.append(frame_prime)\n",
        "        past_frames = past_frames[len(past_frames)-agent.history_length:]\n",
        "        past_frames_last_time = past_frames\n",
        "\n",
        "        # Add to episode history (state, action, reward, state_prime, terminal)\n",
        "        episode_history.add_to_history(\n",
        "                    state, action, reward, state_prime, terminal)\n",
        "        state = state_prime\n",
        "        episode_reward += reward\n",
        "        global_step_counter += 1\n",
        "\n",
        "        #  Do not train predict_model until we have enough\n",
        "        #   episodes in episode history\n",
        "        if global_step_counter > learn_start:\n",
        "          if global_step_counter % train_frequency == 0:\n",
        "              if(len(replay_table.actions) != 0):\n",
        "                q_t = agent.sample_and_train_pred(replay_table, batch_size)\n",
        "                q_t_list.append(q_t)\n",
        "\n",
        "                if global_step_counter % target_frequency == 0:\n",
        "                    agent.model_target.load_state_dict(\n",
        "                        agent.model_predict.state_dict())\n",
        "\n",
        "        # If terminal or max episodes reached,\n",
        "        #   add episode_history to replay table\n",
        "        if j == (max_episode_length - 1):\n",
        "            terminal = True\n",
        "\n",
        "        if terminal:\n",
        "            replay_table.add_episode(episode_history)\n",
        "            episode_rewards.append(episode_reward)\n",
        "            break\n",
        "\n",
        "    print(f'Episode[{i}]: {len(episode_history.actions)} actions, {episode_reward} reward')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mi_y4JxhhTP6",
        "outputId": "8cf21086-dee5-407c-9896-6dde39f1ec0d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode[0]: 232 actions, 1.0 reward\n",
            "Episode[1]: 261 actions, 2.0 reward\n",
            "Episode[2]: 129 actions, -1.0 reward\n",
            "Episode[3]: 234 actions, 1.0 reward\n",
            "Episode[4]: 181 actions, 0.0 reward\n",
            "Episode[5]: 136 actions, -1.0 reward\n",
            "Episode[6]: 219 actions, 1.0 reward\n",
            "Episode[7]: 190 actions, 0.0 reward\n",
            "Episode[8]: 250 actions, 1.0 reward\n",
            "Episode[9]: 137 actions, -1.0 reward\n",
            "Episode[10]: 184 actions, 0.0 reward\n",
            "Episode[11]: 217 actions, 1.0 reward\n",
            "Episode[12]: 140 actions, -1.0 reward\n",
            "Episode[13]: 157 actions, -1.0 reward\n",
            "Episode[14]: 147 actions, -1.0 reward\n",
            "Episode[15]: 241 actions, 1.0 reward\n",
            "Episode[16]: 183 actions, 0.0 reward\n",
            "Episode[17]: 162 actions, -1.0 reward\n",
            "Episode[18]: 164 actions, -1.0 reward\n",
            "Episode[19]: 243 actions, 1.0 reward\n",
            "Episode[20]: 270 actions, 1.0 reward\n",
            "Episode[21]: 162 actions, -1.0 reward\n",
            "Episode[22]: 162 actions, -1.0 reward\n",
            "Episode[23]: 176 actions, -1.0 reward\n",
            "Episode[24]: 284 actions, 2.0 reward\n",
            "Episode[25]: 201 actions, -1.0 reward\n",
            "Episode[26]: 272 actions, -1.0 reward\n",
            "Episode[27]: 340 actions, 0.0 reward\n",
            "Episode[28]: 374 actions, -1.0 reward\n",
            "Episode[29]: 700 actions, 4.0 reward\n",
            "Episode[30]: 455 actions, -1.0 reward\n",
            "Episode[31]: 424 actions, -1.0 reward\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7qvr9aKy_WA5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}